## Bigram Language Model

在自然语言处理中，语言模型的目标是计算一个词元序列 $(w\_1, w\_2, \dots, w\_n)$ 的联合概率 $P(w\_1, w\_2, \dots, w\_n)$。

 1. 精确的概率链式法则

根据概率论的链式法则，这个联合概率可以被精确地分解为一系列条件概率的乘积：

$$P(w_1, w_2, \dots, w_n) = \prod_{i=1}^{n} P(w_i | w_1, w_2, \dots, w_{i-1})$$

这个公式展开后是：

$$P(w_1, \dots, w_n) = P(w_1) \cdot P(w_2|w_1) \cdot P(w_3|w_1, w_2) \cdot \dots \cdot P(w_n|w_1, \dots, w_{n-1})$$

这里 $P(w\_1)$ 是第一个词元出现的概率，可以看作是 $P(w_1 | \text{<start>})$，即在没有任何历史信息的情况下出现的概率。这个公式是精确的，但计算上非常复杂，因为它要求模型考虑非常长的上下文依赖。

2. 二元模型 (Bigram Model) 的简化假设

**二元模型**通过一个很强的马尔可夫假设来简化这个问题。它假设一个词元的出现概率**只依赖于它前面的那一个词元**。

用数学语言表达这个假设就是：

$$P(w_i | w_1, w_2, \dots, w_{i-1}) \approx P(w_i | w_{i-1})$$

 3. 应用简化假设后的联合概率

当我们将这个简化假设应用到原始的概率链式法则上时，整个序列的联合概率的**近似计算**就变成了：

$$P(w_1, w_2, \dots, w_n) \approx \prod_{i=1}^{n} P(w_i | w_{i-1})$$

展开后是：

$$P(w_1, \dots, w_n) \approx P(w_1) \cdot P(w_2|w_1) \cdot P(w_3|w_2) \cdot \dots \cdot P(w_n|w_{n-1})$$

这里的 $P(w\_1)$ 同样可以被看作是 $P(w\_1|w\_0)$，其中 $w\_0$ 是一个虚拟的句子起始符 `<start>`。

## Self-attention

 🔗 1. 注意力是一种通信机制

> "Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights."

这个比喻非常贴切。我们可以把一个序列中的每个词元（token）想象成网络图中的一个**节点**。

-   **通信与信息聚合**: 注意力机制允许每个节点（词元）去“查看”序列中所有其他的节点（包括它自己），并根据相关性决定从其他节点那里“吸收”多少信息。
-   **加权求和 (Weighted Sum)**: 这个“吸收”的过程就是一次加权求和。每个词元的值向量（Value vector, $V$）代表了它自身所携带的信息。而权重（即注意力分数）则决定了在计算一个新词元的表征时，应该在多大程度上考虑其他词元的信息。
-   **数据依赖的权重 (Data-dependent Weights)**: 最关键的一点是，这个权重不是固定的，而是**动态计算**出来的。它取决于查询向量（Query vector, $Q$）和键向量（Key vector, $K$）的相似度。简单来说，一个词元A（提供$Q$）会和所有词元B（提供$K$）计算一个“相关性分数”，这个分数越高，词元A在更新自己时就会更多地“听取”词元B的意见（它的$V$）。

所以，注意力机制本质上是一个动态的、内容相关的**信息路由**或**通信协议**。

---

📏 2. 注意力没有空间概念

> "There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens."

这是自注意力（Self-Attention）的一个根本特性，也是它与 RNN 或 CNN 的主要区别。

-   对于一个自注意力层来说，输入的序列 `[A, B, C]` 和 `[C, B, A]` 是没有区别的，因为它将输入视为一个无序的**集合 (set)**，而不是一个有序的**序列 (sequence)**。它平等地计算任意两个词元之间的关系，而不知道谁前谁后。
-   但在自然语言这类任务中，顺序至关重要（例如，“人咬狗”和“狗咬人”的含义完全不同）。
-   为了解决这个问题，我们需要人为地给模型注入位置信息，这就是 **位置编码 (Positional Encoding)** 的作用。通过在每个词元的输入嵌入向量上加上一个代表其绝对或相对位置的向量，模型就能在计算注意力时同时利用**内容信息**和**位置信息**。

---

🤫 3. 批处理维度是独立的

> "Each example across batch dimension is of course processed completely independently and never 'talk' to each other."

这是一个关于深度学习批处理（Batch Processing）的基本概念，但值得重申。当你向模型输入一个批次的数据（例如，64个不同的句子）时，模型会对这64个句子并行计算。

在注意力机制的计算过程中，第一个句子的词元只会与第一个句子内的其他词元进行交互（计算注意力）。它绝对不会与第二个句子的任何词元产生关联。这保证了数据处理的独立性，并且是利用现代硬件（如 GPU）进行大规模并行计算的基础。

---

🤝 4. Encoder Attention vs. Decoder Attention

> "In an 'encoder' attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a 'decoder' attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling."

这是注意力机制在不同架构中的核心区别。

-   **Encoder Attention (编码器注意力)**: 在编码器中（例如 BERT 或 GPT 的编码器部分），模型的任务是理解整个输入序列的上下文。因此，序列中的任何一个词元都应该能够关注到**所有**其他词元（无论在它前面还是后面）。这种注意力是**双向的**。实现上，就是允许注意力权重矩阵被完全计算，不进行任何遮挡。

-   **Decoder Attention (解码器注意力)**: 在解码器中，尤其是在自回归（autoregressive）语言模型（如 GPT）中，模型的任务是根据已经生成的词元来预测**下一个**词元。为了模拟这个过程，在训练时我们必须防止模型“作弊”。一个在位置 `i` 的词元在预测位置 `i+1` 的词元时，**决不能看到**位置 `i+1` 及之后的内容。
    -   这是通过**掩码 (Masking)** 实现的。通常使用一个下三角矩阵（`tril`）来将注意力权重矩阵中对应未来位置的部分设置为一个极小的值（如 `-inf`），这样在经过 `Softmax` 之后，这些位置的权重就变成了0。这种注意力是**单向的**或**因果的 (causal)**。

---

🔀 5. 自注意力 vs. 交叉注意力

> "'self-attention' just means that the keys and values are produced from the same source as queries. In 'cross-attention', the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)."

-   **自注意力 (Self-Attention)**: 这是我们通常讨论的最基本形式。查询（$Q$）、键（$K$）和值（$V$）这三个关键向量**全部来源于同一个输入序列**。它的作用是让序列内部的词元相互交互，从而重新计算和更新每个词元的表征，使其富含上下文信息。

-   **交叉注意力 (Cross-Attention)**: 这种形式用在需要连接两个不同序列的场景，最典型的就是 Encoder-Decoder 架构（例如机器翻译）。
    -   查询（$Q$）来自于**解码器**的当前状态（代表“我想要什么信息”）。
    -   键（$K$）和值（$V$）来自于**编码器的输出**（代表“我这里有这些信息可供查询”）。
    -   通过交叉注意力，解码器在生成每一个词元时，都可以去“查询”整个输入序列，并决定当前最应该关注输入序列的哪个部分。

---

 ⚖️ 6. 缩放注意力 (Scaled Attention)

> "'Scaled' attention additional divides `wei` by 1/sqrt(`head_size`). This makes it so when input Q,K are unit variance, `wei` will be unit variance too and `Softmax` will stay diffuse and not saturate too much."

这是 Transformer 论文《Attention Is All You Need》中提出的一个关键优化技巧。

注意力分数的计算公式为：
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

-   **问题**: 注意力分数是通过 $Q$ 和 $K$ 的点积计算得到的。假设 $Q$ 和 $K$ 的分量是均值为0、方差为1的独立随机变量，那么它们的点积 $QK^T$ 的均值为0，但方差会是 $d_k$（即 $K$ 向量的维度，也等于 `head_size`）。
-   **影响**: 如果 $d_k$ 很大（例如 64 或 128），那么点积结果的方差就会很大，这意味着一些点积的值会非常大，而另一些会非常小。当这些值被送入 `Softmax` 函数时，`Softmax` 的输出会**饱和 (saturate)**——即梯度会变得极其接近于0。一个输出会无限趋近于1，而其他的会无限趋近于0。这会导致模型在训练初期就难以学习，梯度消失，训练不稳定。
-   **解决方案**: 将点积的结果除以 $\sqrt{d_k}$。这个“缩放”操作可以将点积结果的方差重新拉回到 1 左右，从而使得 `Softmax` 的输入分布在一个更“温和”的范围内，避免其过早饱和，保证了梯度能够有效回传，让训练过程更加稳定。


>缩放点积注意力（Scaled Dot-Product Attention）为何需要缩放时非常关键的数学证明

**问题陈述**

给定两个 $d_k$ 维的向量 $Q$ 和 $K$：
$Q = [q_1, q_2, \dots, q_{d_k}]$
$K = [k_1, k_2, \dots, k_{d_k}]$

**假设**:
1.  $Q$ 和 $K$ 的所有分量 $q_i$ 和 $k_j$ 都是相互独立的随机变量。
2.  所有分量的期望（均值）都为 0，即 $E[q_i] = 0$ 且 $E[k_i] = 0$ 对于所有 $i$ 成立。
3.  所有分量的方差都为 1，即 $Var(q_i) = 1$ 且 $Var(k_i) = 1$ 对于所有 $i$ 成立。

**目标**:
证明它们的点积 $S = QK^T = \sum_{i=1}^{d_k} q_i k_i$ 的期望为 0，方差为 $d_k$。
即证明：
1.  $E[S] = 0$
2.  $Var(S) = d_k$

---

 **概率论公式推导**

**1. 推导均值 (Expectation)**

我们首先计算点积 $S$ 的期望 $E[S]$。

$$E[S] = E\left[\sum_{i=1}^{d_k} q_i k_i\right]$$

根据期望的线性性质，$E[X+Y] = E[X] + E[Y]$，我们可以将求和符号提到期望的外面：

$$E[S] = \sum_{i=1}^{d_k} E[q_i k_i]$$

根据我们的假设，$q_i$ 和 $k_i$ 是相互独立的随机变量。对于相互独立的变量，其乘积的期望等于各自期望的乘积，即 $E[XY] = E[X]E[Y]$。因此：

$$E[q_i k_i] = E[q_i] \cdot E[k_i]$$

代入我们已知的假设 $E[q_i] = 0$ 和 $E[k_i] = 0$：

$$E[q_i k_i] = 0 \cdot 0 = 0$$

将这个结果代回到求和式中：

$$E[S] = \sum_{i=1}^{d_k} 0 = 0$$

**结论：点积的均值为 0。**

---

**2. 推导方差 (Variance)**

接下来，我们计算点积 $S$ 的方差 $Var(S)$。
方差的定义是 $Var(X) = E[X^2] - (E[X])^2$。

因为我们已经证明了 $E[S] = 0$，所以方差的计算可以简化为：

$$Var(S) = E[S^2] - (E[S])^2 = E[S^2] - 0^2 = E[S^2]$$

现在我们的目标是计算 $E[S^2]$。

$$S^2 = \left(\sum_{i=1}^{d_k} q_i k_i\right)^2 = \left(\sum_{i=1}^{d_k} q_i k_i\right) \left(\sum_{j=1}^{d_k} q_j k_j\right)$$
注意这里我们使用不同的索引 $i$ 和 $j$ 来展开平方，这对于后续的分析至关重要。

$$S^2 = \sum_{i=1}^{d_k} \sum_{j=1}^{d_k} q_i k_i q_j k_j$$

现在我们对 $S^2$ 求期望：

$$E[S^2] = E\left[\sum_{i=1}^{d_k} \sum_{j=1}^{d_k} q_i k_i q_j k_j\right] = \sum_{i=1}^{d_k} \sum_{j=1}^{d_k} E[q_i k_i q_j k_j]$$

为了计算 $E[q_i k_i q_j k_j]$，我们需要分两种情况讨论：

**情况一：$i = j$**
当 $i = j$ 时，求和项变为 $E[q_i k_i q_i k_i] = E[q_i^2 k_i^2]$。
由于所有分量相互独立，所以 $q_i^2$ 和 $k_i^2$ 也相互独立。因此：
$E[q_i^2 k_i^2] = E[q_i^2] \cdot E[k_i^2]$
我们知道 $Var(q_i) = E[q_i^2] - (E[q_i])^2$。因为 $Var(q_i)=1$ 且 $E[q_i]=0$，所以 $E[q_i^2] = 1$。
同理可得 $E[k_i^2] = 1$。
所以，当 $i=j$ 时，
$E[q_i^2 k_i^2] = 1 \cdot 1 = 1$。

**情况二：$i \neq j$**
当 $i \neq j$ 时，求和项为 $E[q_i k_i q_j k_j]$。
根据我们的假设，这四个随机变量 $q_i, k_i, q_j, k_j$ 是相互独立的。因此，我们可以将期望拆分为四个独立期望的乘积：
$E[q_i k_i q_j k_j] = E[q_i] \cdot E[k_i] \cdot E[q_j] \cdot E[k_j]$
代入已知均值：
$E[q_i k_i q_j k_j] = 0 \cdot 0 \cdot 0 \cdot 0 = 0$。

现在，我们将这两种情况的结果代回到双重求和中。这个双重求和可以被拆分为对角线项（$i=j$）和非对角线项（$i \neq j$）的和：

$$E[S^2] = \sum_{i=j} E[q_i k_i q_j k_j] + \sum_{i \neq j} E[q_i k_i q_j k_j]$$

$$E[S^2] = \sum_{i=1}^{d_k} E[q_i^2 k_i^2] + \sum_{i \neq j} (0)$$

$$E[S^2] = \sum_{i=1}^{d_k} (1) + 0$$

这个求和式子就是将 $d_k$ 个 1 相加：

$$E[S^2] = d_k$$

因为 $Var(S) = E[S^2]$，所以我们得到：

$$Var(S) = d_k$$

**结论：点积的方差为 $d_k$。**

---

 **总结**

我们已经通过数学推导证明了，当 $Q$ 和 $K$ 的分量是均值为0、方差为1的独立随机变量时，它们的点积 $QK^T$ 的均值为0，方差为 $d_k$。

这个结论解释了为什么在 Transformer 中需要进行**缩放**：点积的方差会随着向量维度 $d_k$ 的增加而线性增长。如果不进行缩放，当 $d_k$ 很大时，点积结果的数值会非常分散，导致 `Softmax` 函数进入梯度非常小的饱和区，从而使得模型训练困难。通过将点积除以 $\sqrt{d_k}$，可以将结果的方差重新调整回 1，保证了训练的稳定性。（这里的方差主要影响的是稳定性）

### `BoW`
**词袋模型 (Bag-of-Words, `BoW`)**。

这是一个在自然语言处理（NLP）和信息检索（IR）领域中，非常基础且重要的文本表示方法。它的核心作用是将非结构化的文本转换为计算机能够处理的、结构化的数值型向量。

🧠 核心思想

词袋模型的名字非常形象。想象你有一个不透明的袋子，你将一篇文章或一个句子中的所有词语（分词后）都扔进这个袋子里，然后用力摇晃。在这个过程中，词语原本的顺序、语法结构全都被打乱了。最后，你从袋子里统计每个词语出现了多少次。

**词袋模型的核心思想就是：**

> **忽略文本的语法和词序，仅仅将其看作是一个由词语组成的“无序集合”（或“袋子”），然后通过统计每个词语在文本中出现的频率来表示该文本。**

一个文档的向量表示中，每个维度代表一个特定的词，而该维度上的值则表示这个词在该文档中出现的频次。

---

 ⚙️ 工作流程

构建词袋模型通常遵循以下四个步骤：

 **第一步：收集文本语料 (Text Collection)**

首先，需要一个包含一个或多个文档的语料库（Corpus）。

例如：

- `文档1: "我喜欢看电影，也喜欢听音乐。"`
    
- `文档2: "我不喜欢看电影，但我喜欢看书。"`
    

 **第二步：分词 (Tokenization)**

对语料库中的每个文档进行分词，将其分解成一个个独立的词元（token）。通常还会进行一些预处理，如转为小写、去除标点符号、去除停用词（stop words，如“的”、“也”、“是”等）等。

分词后：

- `文档1`: `["我", "喜欢", "看", "电影", ",", "也", "喜欢", "听", "音乐", "。"]`
    
- `文档2`: `["我", "不", "喜欢", "看", "电影", ",", "但", "我", "喜欢", "看", "书", "。"]`
    

去除标点和停用词后（假设“我”、“也”、“但”、“不”等是停用词，这里为了演示保留部分）：

- `文档1`: `["喜欢", "看", "电影", "喜欢", "听", "音乐"]`
    
- `文档2`: `["不", "喜欢", "看", "电影", "喜欢", "看", "书"]`
    

 **第三步：构建词汇表 (Vocabulary Creation)**

从整个语料库中所有文档的分词结果中，创建一个包含所有不重复词语的“词汇表”。词汇表中的每个词语都会被赋予一个唯一的索引。

基于上面的例子，我们的词汇表可能是：

{"喜欢": 0, "看": 1, "电影": 2, "听": 3, "音乐": 4, "不": 5, "书": 6}

词汇表的大小（维度）为 7。

 **第四步：向量化 (Vectorization)**

现在，我们可以根据构建好的词汇表，将每个原始文档转换为一个数值向量。向量的维度就是词汇表的大小。向量中每个位置的值，代表了对应词汇表中该词语在当前文档中出现的次数。

- **对于文档1 `["喜欢", "看", "电影", "喜欢", "听", "音乐"]`:**
    
    - `喜欢` (索引0) 出现了 2 次
        
    - `看` (索引1) 出现了 1 次
        
    - `电影` (索引2) 出现了 1 次
        
    - `听` (索引3) 出现了 1 次
        
    - `音乐` (索引4) 出现了 1 次
        
    - `不` (索引5) 出现了 0 次
        
    - `书` (索引6) 出现了 0 次
        
    - **文档1的词袋向量为: `[2, 1, 1, 1, 1, 0, 0]`**
        
- **对于文档2 `["不", "喜欢", "看", "电影", "喜欢", "看", "书"]`:**
    
    - `喜欢` (索引0) 出现了 2 次
        
    - `看` (索引1) 出现了 2 次
        
    - `电影` (索引2) 出现了 1 次
        
    - `不` (索引5) 出现了 1 次
        
    - `书` (索引6) 出现了 1 次
        
    - ...其他词出现 0 次
        
    - **文档2的词袋向量为: `[2, 2, 1, 0, 0, 1, 1]`**
        

至此，我们就成功地将两段文本转换成了计算机可以处理的数值向量。

---

 📈 模型变种

基本的词袋模型只计算词频，但有一些常见的改进方法：

1. TF-IDF (Term Frequency-Inverse Document Frequency)
    
    这是对词袋模型最经典的改进。它不仅仅考虑一个词在当前文档中的频率（TF），还考虑它在整个语料库中的稀有程度（IDF）。
    
    - **TF (词频)**: 与词袋模型一样，指一个词在文档中出现的频率。
        
    - **IDF (逆文档频率)**: 如果一个词在很多文档中都出现，说明它很普遍，重要性较低（如“的”、“是”），其IDF值就低。反之，如果一个词只在少数几个文档中出现，说明它很独特，重要性较高，其IDF值就高。
        
    - **公式**: TF−IDF(词,文档)=TF(词,文档)×IDF(词)
        
    - 用TF-IDF值来代替简单的词频计数，可以更好地突出那些对区分文档内容有重要作用的关键词。
        
2. N-gram 模型
    
    基本的词袋模型丢失了所有词序信息。N-gram模型通过将连续的N个词作为一个单元（token）来部分地保留局部语序。
    
    - **Bigram (2-gram)**: `["喜欢 看", "看 电影", ...]`
        
    - **Trigram (3-gram)**: `["我 喜欢 看", "喜欢 看 电影", ...]`
        
    - 通过这种方式，“不喜欢”可以被看作一个整体，与“喜欢”区分开，部分解决了词序问题。
        

---

 👍 👎 优点与缺点

 **优点**

1. **简单直观**：模型原理简单，易于理解和实现。
    
2. **计算高效**：向量化过程非常快。
    
3. **效果尚可**：对于一些不那么依赖语序的文本分类、情感分析等任务，词袋模型常常能取得不错的效果。
    

 **缺点**

1. **忽略词序**：这是其最致命的弱点。例如，“狗咬人”和“人咬狗”在词袋模型看来是完全一样的，因为它们的词频向量相同。
    
2. **无法体现语义**：模型无法理解词语之间的语义关系。例如，“国王”和“女王”在模型看来是两个完全不相关的词，尽管它们在语义上很接近。
    
3. **维度灾难与稀疏性**：当词汇表非常大时（在真实世界任务中可能有几十万甚至上百万个词），文档向量的维度会变得非常高。而对于单个文档，其中大部分维度的值都会是0，造成了**数据稀疏性**，这给后续模型的计算和存储带来挑战。
    

---

 🚀 应用场景

尽管有其局限性，词袋模型（尤其是其TF-IDF变种）在许多场景中仍然有用：

- **文档分类**：如新闻分类、垃圾邮件过滤、情感分析。
    
- **信息检索**：计算查询语句与文档之间的相似度，返回最相关的结果。
    
- **主题建模**：如LDA（Latent Dirichlet Allocation）等模型就基于词袋假设。
    

 **总结**

词袋模型是NLP历史上一个里程碑式的思想，它开创了将文本“数值化”和“向量化”的先河。虽然现在更先进的模型（如基于神经网络的 Word2Vec、GloVe，以及基于Transformer的BERT等）能够更好地捕捉语序和语义信息，在很多任务上表现更优，但词袋模型依然是理解文本向量化、信息检索和机器学习基础的重要第一步。